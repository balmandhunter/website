<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Data</title>

    <!-- Bootstrap Core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="css/clean-blog.min.css" rel="stylesheet">
    <link rel="stylesheet" href="css/custom.css" type="text/css">
    <link rel="stylesheet" href="css/animate.min.css" type="text/css">

    <!-- Custom Fonts -->
    <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">

    <link href='http://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>

    <link href='http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>


    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>

<body>

    <!-- Navigation -->
    <nav id="mainNav" class="navbar navbar-default navbar-fixed-top">
        <div class="container-fluid">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header">
                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav navbar-left">
                    <li>
                        <a class="page-scroll" href="/">Home</a>
                    </li>
                </ul>
                <ul class="nav navbar-nav navbar-right">

                    <li>
                        <a class="page-scroll" href="/#portfolio">Interests</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="/#contact">Contact</a>
                    </li>
                    <li>
                        <a href="cv.pdf" class="page-scroll" href="#resume">R&eacute;sum&eacute;</a>
                    </li>
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container-fluid -->
    </nav>

    <!-- Page Header -->
    <!-- Set your background image for this header on the line below. -->
    <header class="intro-header" style="background-image: url('img/data_header.jpg')">

        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                    <div class="site-heading">
                        <h1>Data</h1>
                        <hr class="small">
                        <span class="subheading">a sea of information</span>
                    </div>
                </div>
            </div>
        </div>
    </header>

    <!-- Main Content -->
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                    <div class="post-preview">
                            <p>
                                Why am I a data junkie? Because I'm  excited about getting my hands on the vast reservoir of information that is waiting to be tapped. As scientists and engineers, we spend most of our time acquiring information through research and experimentation. Data science capitalizes on the opportunity to extract information from the mountains of data that have already been collected. 
                            </p>
                            <p>
                                I've recently experienced this phenomenon in my work as a PhD candidate at the University of Colorado. My research group develops inexpensive air quality monitors using low-cost sensors, and lately, I've been exploring how machine learning can improve sensor performance. In  a few short months, I have been able to significantly improve the regression models that we use to calibrate our sensors, and therefore improve the accuracy of our measurements- all without spending any time improving instrument design or performing more experiments. 
                            </p>
                            <p>
                                The ability of data science to exponentially accelerate the pace of research is very exciting, and can be applied to almost any field. I am eager to explore the hidden possibilities of data, and push the boundaries of understanding. 
                            </p>
                    </div>

                    <hr class = "big">
                        <h2> Personal Work - Exploring the Spread of the Tour de France 
                        </h2>

                        <p>
                        </p>

                        <div class="post-preview">
                        <a href="tdf/tour_de_france.html" class="portfolio-box" target="_blank">
                            <p>
                            I've been pretty obsessed with following the Tour de France this July, and I thought it would be fun to use D3 to create an interactive graphic that shows the how participation in the Tour has changed since inaugural event in 1903. I hope you enjoy exploring the graphic as much as I enjoyed developing it!
                            </p> 
                            <img src="img/data18.png" class="img-responsive center-block max_width" alt="">   
                            <p>
                            (animation currently only works in Google Chrome)    
                        </a>
                        </div>
                     
                    <hr class = "big">


                     <h2> Personal Work - Playing with D3 (in progress...)
                        </h2>
                         <div id="chart3_d3" class="img-responsive center-block space max_width"></div>

                       
                    <hr class = "big">
 
                    <div class="post-preview">                            
                        <h2> Personal Work - Regression Modeling for Sensor Calibration
                        </h2>

 

                        <p>
                            I have always valued explanatory data visualization, but the importance of exploratory data visualization has become more apparent to me as I have worked on improving the regression models we use for sensor calibrations. Check out my IPython notebook on <a class = "neutral" href="https://github.com/balmandhunter/Regressions_machine_learning/blob/master/CAMP_calibrations.ipynb">GitHub</a> to see all of the visualizations I used to guide my regression modeling decisions.
                        </p>
                        <img src="img/data3.png" class="img-responsive center-block space max_width" alt="">
                        <p>
                            The plot above is a comparison between the ozone values predicted by a linear regression model, using the data from the low-cost sensors, and the ozone values measured by a reference instrument. The cross-validation root-mean-square error (RMSE) for this model is 6&thinsp;ppb, which means that the mean difference  between the reference ozone concentration measurements and the model predictions is 6&thinsp;ppb.  This RMSE is very good, especially when you consider the fact that we're comparing the performance of a $10 sensor to a $10,000+ reference instrument. However, when you look at this plot, you can see that at high ozone values (>&thinsp;60&thinsp;ppb), the model underpredicts ozone by an average of 10.4 ppb, with an RMSE of 11&thinsp;ppb. Since exposure to high ozone concentrations can negatively impact health, researchers are most concerned about these high ozone values, and it is critical to measure them accurately. Without plotting this data, one could conclude that this model is a great fit, but this plot instantly makes it clear that it does not work well at high values.
                        </p>

                        <img src="img/data10.png" class="img-responsive center-block space max_width" alt="">
                        <img src="img/data9.png" class="img-responsive center-block space max_width" alt="">
                        <p>
                            You can observe the distribution of the predicted and reference data above. The blue lines represent how the data would look if it were perfectly normally distributed, and looking at the actual data distribution in the bins shows us that the data is close to normally distributed. The top plot represents the distribution of the reference data. There is an abrupt cutoff at a concentration of zero, since ozone concentration cannot be less than zero. Notice that when the ozone concentration is above 50&thinsp;ppb, the bins match the normal distribution very closely. 
                            The ozone concentration predictions in bottom plot, on the other hand, abruptly drop off around 60&thinsp;ppb, and there are no predicted concentrations above 65&thinsp;ppb. 
                        </p>

                        <img src="img/data4.png" class="img-responsive center-block space max_width" alt=""> 
                        <p>
                            To reduce the model's underprediction at high values, I transformed the base features (ozone sensor signal, temperature and humidity) to create 321 new features. The plot above shows a linear regression with all 324 features. Adding the new features to the regression reduced the magnitude of the underprediction from at high values to 2.6&thinsp;ppb, and the high-ozone cross-validation RMSE decreased slightly to 10&thinsp;ppb. As expected when adding a large number of features, the variance increased, and the overall RMSE increased from 6&thinsp;ppb to 7&thinsp;ppb. 
                        </p>

                        <img src="img/data12.png" class="img-responsive center-block space max_width" alt="">
                        <img src="img/data11.png" class="img-responsive center-block space max_width" alt="">
                       
                        <p>
                            The histograms for the linear model with the new features show much better predictions at high values. I removed the normal distribution lines from these plots to improve the visilibility of high ozone concentrations. While these histograms look good, they are based on a model with all 324 features. Since it is always best to use the simplest model that produces good results, the next step is to try to simplify the model. Reducing the number of features in a model is also one way to reduce variance in the model. 
                        </p>



                        <p>
                        I used forward selection to reduce the number of features in the model. I wrote an algorithm that chooses each feature based on minimizing a custom error function. The custom function is:
                        <br><br>
                        <img src="img/data15.png" class="img-responsive center-block space max_width" alt=""> 

                        <p>
                        where C<sub>m,r</sub> is the median of the reference ozone data when the ozone concentration is <br>>&thinsp;60&thinsp;ppb, C<sub>m,p</sub> is the median of the predicted ozone data for high reference ozone, MSE<sub>h</sub> is the MSE at high ozone values, and MSE<sub>l</sub> is the MSE at low ozone values. 
                        </p>
                        <img src="img/data1.png" class="img-responsive center-block space max_width" alt=""> 
                        <img src="img/data13.png" class="img-responsive center-block space max_width" alt=""> 
                        <p>
                         The plots above show how the custom error and RMSE change as each next-best feature is added to the regression. The top plot shows the 100 best features, and the bottom plot is zoomed in on the area in the red circle. Based on this data, I will likely select between 33 and 37 features for the model, since both the custom score and overall RMSE are low in this region. In addition to looking at the plots above, I looked at learning curves to evaluate when adding features starts to lead to overfitting. 
                        </p>

                        <img src="img/data5.png" class="img-responsive center-block space max_width" alt=""> 
                        <p>
                        Learning curves visualize how error for a cross-validation set and training set change as the number of training samples increases. When a fit has high bias, the training and cross-validation error converge at high error values. When a fit has high variance, as the number of samples increases, the gap between the training error and cross-validation error remains large. The plot above is a learning curve for a fit that includes all 324 features. You can see that there is a large difference between the training and cross-validation error, which is a sign of high variance.
                        </p>

                        <img src="img/data6.png" class="img-responsive center-block space max_width" alt=""> 
                        <p>
                        I plotted a learning curve for each feature that I added to the model. By looking through the curves, and seeing how they change with each feature, you can tell if and when adding more features leads to overfitting. The learning curve above is for the maximum number of features I'm considering (37). The training and cross-validation lines are close together, so the model does not have a variance problem, and including up to 37 features in the model should not lead to overfitting. See my IPython notebook on <a class = "neutral" href="https://github.com/balmandhunter/Regressions_machine_learning/blob/master/CAMP_calibrations.ipynb">GitHub</a> to view the learning curves for the best 100 features. 
                        </p>

                        <img src="img/data16.png" class="img-responsive center-block space max_width" alt=""> 
                        <p>
                        Based on the plots discussed above and the statistical results, I decided to use 36 features for my model. The plot above shows a comparison between the ozone values predicted by a the new linear regression model, and the values measured by the reference instrument. The overall cross-validation RMSE is 4&thinsp;ppb, the high-value RMSE is 6&thinsp;ppb, and the difference in high-value medians is -1.9&thinsp;ppb.  
                        </p>

                        <img src="img/data12.png" class="img-responsive center-block space max_width" alt="">
                        <img src="img/data17.png" class="img-responsive center-block space max_width" alt="">
                        <p>
                        The histograms above show how well the distribution of the predicted data matches the reference data. 
                        </p>

                        <img src="img/data7.png" class="img-responsive center-block space max_width" alt=""> 
                        <img src="img/data8.png" class="img-responsive center-block space max_width" alt=""> 

                        <p>
                        Another method for reducing variance is to reduce the number of features in a model (and/or the magnitude of their coefficients) using a shrinkage penalty. The plots above are visualizations that aid in the selection of the shrinkage parameter, lambda, for the <a class = "neutral" href="http://statweb.stanford.edu/~tibs/lasso/lasso.pdf">lasso</a> reduction method. The higher the value of lambda, the larger the penalty placed on high coefficient values. The top plot visualizes how the coefficients change as lambda is increased, and the bottom plot shows how the custom error function I defined varies with lambda. The optimal lambda value is the one that results in the lowest error. The overall cross-validation RMSE for the Lasso model is 5&thinsp;ppb, the high-value RMSE is 6&thinsp;ppb, and the difference in high-value medians is -3.7&thinsp;ppb. While these statistics are significantly better than the base model, they are not as good as the forward selection results.
                        </p>

                        <p>
                        The best results for the cross-validation data were obtained using a linear model with features selected using forward selection with a custom error function. In addition to cross-validation data, it is important to have a true holdout data set, which is not used to choose the best model. For the true holdout data, using the best model, the overall RMSE is 3&thinsp;ppb, the high-value RMSE is 4&thinsp;ppb, and the difference in high-value medians is -2.2&thinsp;ppb.
                        </p> 

                        <p>
                        Other methods used in this experiment include random forest regressions and support vector machines. See my <a class = "neutral" href="https://github.com/balmandhunter/Regressions_machine_learning/blob/master/CAMP_calibrations.ipynb">GitHub</a> for more information.

                    </div>


                    <hr class = "big">
                     
                    <div class="post-preview">
                        <a href="http://www.bbc.co.uk/programmes/p00cgkfk">
                            
                        <h2> "Hans Rosling's 200 countries, 200 years, 4 minutes" 
                        </h2> 
                        <p class = "gray">
                        (source: BBC)
                        </p>
                        <p>
                            Hans Rosling's augmented reality data visualizations are absolutely incredible. 
                         </p>
                            <div class="embed-responsive embed-responsive-16by9 center-block max_width">  
                            <iframe width="640" height="360" src="https://www.youtube.com/embed/jbkSRLYSojo?rel=0" frameborder="0" allowfullscreen></iframe>
                            </div>


                            <p>
                            "Hans Rosling's famous lectures combine enormous quantities of public data with a sport's commentator's style to reveal the story of the world's past, present and future development. Now he explores stats in a way he has never done before - using augmented reality animation. In this spectacular section of 'The Joy of Stats' he tells the story of the world in 200 countries over 200 years using 120,000 numbers - in just four minutes. Plotting life expectancy against income for every country since 1810, Hans shows how the world we live in is radically different from the world most of us imagine."
                            [click for more information]
                            </p>    

                        </a>
                    </div>


                    <hr class = "big">
                        <div class="post-preview">
                        <a href="http://www.r2d3.us/visual-intro-to-machine-learning-part-1/">
                            <h2> "A Visual Introduction to Machine Learning"
                            </h3>
                            <p class = "gray">
                               (source: R2D3)
                            </p>  
                            <img src="img/interesting1.png" class="img-responsive center-block max_width" alt="">   
                            <p>
                            "In machine learning, computers apply statistical learning techniques to automatically identify patterns in data. These techniques can be used to make highly accurate predictions.

                            Using a data set about homes, we will create a machine learning model to distinguish homes in New York from homes in San Francisco...."
                            [click to continue reading]
                            </p>            
                        </a>
                    </div>
                    <hr class = "big">

                    <div class="post-preview">
                        <a href="http://technology.stitchfix.com/blog/2015/01/20/eda-and-graphics/">
                            
                        <h2> "Exploratory Data Analysis and Graphics" 
                        </h2> 
                        <p class = "gray">
                        (source: Stitch Fix Technology Blog)
                        </p>
                        <p>
                            I'm intrigued by data visualization. I believe that superb visualizations represent an intersection between art, science and education. The article, below, explains how even simple data visualizations can provide a wealth of information:
                         </p>

                            <img src="img/data_vis.png" class="img-responsive center-block max_width" alt=""> 

                            <p>
                            "In data science, or any related quantitative field, we strive to understand and leverage our data for our objectives. These data will usually be part of a bigger project that we’re working on where the work flow looks something like the following..."
                            [click to continue reading]
                            </p>    

                        </a>
                    </div>
                    <hr class = "big">
                    <div class="post-preview">
                        <a href="http://fivethirtyeight.com/interactives/flights/"> 
                            <h2> "Which Flight Will Get You There Fastest?"
                            </h2>
                            <p class="gray">
                               (source: FiveThirtyEight)
                            </p> 
                            <p>
                            Nate Silver made statistics cool during the 2008 elections. What more needs to be said?
                            </p> 
                            <img src="img/flights.jpg" class="img-responsive center-block max_width" alt="">   
                            <p>
                            "FiveThirtyEight analyzed 6 million flights to figure out which airports, airlines and routes are most likely to get you there on time and which ones will leave you waiting..."
                            [click to continue reading]
                            </p>            
                        </a>
                    </div>
                    <hr class = "big">
                    <div class="post-preview">
                        <a href="http://fivethirtyeight.com/interactives/marriage-penalty/">
                            <h2> "Should You Get Married (Or Divorced) For Tax Reasons?"
                            </h3>
                            <p class = "gray">
                               (source: FiveThirtyEight)
                            </p>  
                            <img src="img/tax_marriage.png" class="img-responsive center-block max_width" alt="">   
                            <p>
                            "I was super excited about doing my taxes this year. I got married last June and couldn’t wait to exercise that most glorious privilege of matrimony — the ability to file jointly.

                            But after my wife and I filled out our tax return, I wondered … did we actually save any money, or did we, like an estimated 38 percent of couples, end up having to pay more because of our new legal status?..."[click to continue reading]
                            </p>            
                        </a>
                    </div>
                    <hr class = "big">
                    <div class="post-preview">
                        <a href="http://www.nytimes.com/interactive/2014/08/13/upshot/where-people-in-each-state-were-born.html?abt=0002&abg=1">
                            <h2> "Where We Came From and Where We Went, State by State"
                            </h3>
                            <p class = "gray">
                               (source: New York Times)
                            </p>  
                            <p>
                                I have a confession to make. I have a massive visualization crush on the data team at the New York Times. They somehow manage to repeatedly produce interactive data visualizations that are engaging, relevant, pragmatic, and beautiful. I aspire to create such masterpieces during my career. 
                            </p> 
                            <img src="img/born_in_CA.png" class="img-responsive center-block max_width" alt="">   
                            <p>
                            "Foreign immigration is a hot topic these days, but the movement of people from one state to another can have an even bigger influence on the United States’ economy, politics and culture. Americans have already seen this with the Western expansion, the movement of Southern blacks to Northern cities and the migration from the Rust Belt..."
                            [click to continue reading]
                            </p>            
                        </a>
                    </div>
                    <hr class = "big">
                    <div class="post-preview">
                        <a href="http://www.nytimes.com/interactive/2014/upshot/buy-rent-calculator.html?abt=0002&abg=1">
                            <h2> "Is it better to rent or buy?"
                            </h3>
                            <p class = "gray">
                               (source: New York Times)
                            </p>  
                            <img src="img/home_price.png" class="img-responsive center-block max_width" alt="">   
                            <p>
                            "The choice between buying a home and renting one is among the biggest financial decisions that many adults make. But the costs of buying are more varied and complicated than for renting, making it hard to tell which is a better deal. To help you answer this question, our calculator takes the most important costs associated with buying a house and computes the equivalent monthly rent..."
                            [click to continue reading]
                            </p>            
                        </a>
                    </div>


                </div>
            </div>
        </div>
    <hr>

    <!-- Footer -->
    <footer>
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                    <ul class="list-inline text-center">
                    </ul>
                    <p class="copyright text-muted">Copyright &copy; Berkeley Almand-Hunter 2015</p>
                </div>
            </div>
        </div>
    </footer>

</body>

<!-- jQuery -->
    <script src="js/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="js/bootstrap.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="js/jquery.easing.min.js"></script>
    <script src="js/jquery.fittext.js"></script>
    <script src="js/wow.min.js"></script>

    <!-- Custom Theme JavaScript -->
    <script src="js/clean-blog.min.js"></script>
    <script src="http://d3js.org/d3.v3.min.js"></script>
    <script src="http://dimplejs.org/dist/dimple.v2.0.0.min.js"></script>
    <script src="js/plot3.js"></script>

</html>
Status API Training Shop Blog About Help
© 2015 GitHub, Inc. Terms Privacy Security Contact
